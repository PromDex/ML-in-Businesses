{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Урок 2. #Профилирование пользователей. Сегментация аудитории: unsupervised learning (clustering, LDA/ARTM), supervised (multi/binary classification)#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание\n",
    "1. Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "3. Повторить п.2, но используя уже не медиану, а max\n",
    "4. (опциональное, если очень хочется) Воспользовавшись полученными знаниями из п.1, повторить пункт 2, но уже взвешивая новости по tfidf (подсказка: нужно получить веса-коэффициенты для каждого документа. Не все документы одинаково информативны и несут какой-то положительный сигнал). Подсказка 2 - нужен именно idf, как вес.\n",
    "5. Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score\n",
    "6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[переход к ДЗ](#label_of_your_choice) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='## Домашнее задание'></a>\n",
    "<!--ссылка в файле experiment00.ipynb-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import itertools\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "from razdel import tokenize\n",
    "import pymorphy2\n",
    "\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, \\\n",
    "                                precision_recall_curve, confusion_matrix\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "загрузим списки: новостей и пользователей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(27000, 2)\n",
      "(8000, 2)\n",
      "(8000, 2)\n"
     ]
    }
   ],
   "source": [
    "news = pd.read_csv(\"articles.csv\")\n",
    "users = pd.read_csv(\"users_articles.csv\")\n",
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "print(news.shape)\n",
    "print(users.shape)\n",
    "print(target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Наши новости , пользователи и списки последних прочитанных новостей."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>Заместитель председателяnправительства РФnСерг...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>Матч 1/16 финала Кубка России по футболу был п...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>Форвард «Авангарда» Томаш Заборский прокоммент...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id                                              title\n",
       "0       6  Заместитель председателяnправительства РФnСерг...\n",
       "1    4896  Матч 1/16 финала Кубка России по футболу был п...\n",
       "2    4897  Форвард «Авангарда» Томаш Заборский прокоммент..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u107120</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u102277</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u102444</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid  churn\n",
       "0  u107120      0\n",
       "1  u102277      0\n",
       "2  u102444      0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(news.head(3), users.head(3), target.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Итак, нам нужно получить векторные представления пользователей на основе прочитанным ими новостей и самих новостей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Получаем векторные представления новостей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#из gensim.test.utils импортировать common_texts\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "#Библиотека gensim.corpora.dictionary в Python является частью фреймворка gensim, который используется для работы с корпусами текстов, тематическим моделированием и векторными представлениями слов.\n",
    "#Модуль Dictionary этой библиотеки предназначен для создания словаря из набора текстовых документов. Словарь содержит уникальные слова из текстов и их идентификаторы.\n",
    "#Таким образом, этот модуль может использоваться для создания словарей, которые затем могут быть использованы для построения моделей тематического моделирования, поиска похожих документов и классификации текстов.\n",
    "#Импортируя модуль Dictionary из библиотеки gensim.corpora.dictionary, можно использовать его функционал для создания словаря из текстовых документов и дальнейшей работы с ним в соответствии с поставленными задачами."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#предобработка текстов\n",
    "import re\n",
    "#Библиотека re в Python предоставляет возможность работы с регулярными выражениями. Она используется для работы с текстовыми строками и позволяет осуществлять поиск, замену и манипуляции с текстом на основе определенных шаблонов.\n",
    "#Импортируя эту библиотеку с помощью команды import re, вы можете использовать ее функционал для выполнения различных операций с текстом, например, для:\n",
    "#Поиска подстроки в тексте\n",
    "#Замены определенных символов или последовательностей символов в тексте\n",
    "#Разбиения текста на подстроки по определенному разделителю\n",
    "#Извлечения определенных значений из текста с помощью регулярных выражений\n",
    "#Проверки соответствия текста определенному шаблону\n",
    "#Кроме того, библиотека re широко используется в области обработки естественного языка (Natural Language Processing - NLP), например, для предобработки текста перед его анализом и классификацией.\n",
    "\n",
    "from nltk.corpus import stopwords # так почему то не сработало\n",
    "#Библиотека nltk (Natural Language Toolkit) в Python предоставляет инструменты для обработки естественного языка (Natural Language Processing - NLP).\n",
    "#Она используется для работы с текстовыми данными и включает в себя различные модули.\n",
    "#Модуль stopwords этой библиотеки содержит список стоп-слов для различных языков, которые обычно не несут важной смысловой нагрузки в тексте и могут быть исключены из анализа.\n",
    "#Это позволяет снизить размерность данных и повысить качество анализа.\n",
    "#Импортируя модуль stopwords из библиотеки nltk.corpus, вы можете использовать его функционал для удаления стоп-слов из текста перед его анализом или классификацией.\n",
    "#Для этого можно использовать методы этого модуля, такие как words() или fileids(), чтобы получить список стоп-слов для определенного языка, а затем удалить их из текста с помощью простых операций в Python.\n",
    "#import nltk\n",
    "#nltk.download()\n",
    "#Если библиотека nltk была установлена, но все равно возникла ошибка, то проблема может быть связана с отсутствием данных, которые необходимы для работы библиотеки.\n",
    "#В этом случае необходимо выполнить дополнительные шаги, чтобы загрузить необходимые данные, например, загрузить корпусы текстов с помощью команды nltk.download().\n",
    "\n",
    "\n",
    "from razdel import tokenize #!pip install razdel\n",
    "#https://github.com/natasha/razdel\n",
    "#Библиотека razdel в Python является инструментом для токенизации русскоязычных текстов.\n",
    "#Она предоставляет функционал для разбиения текста на отдельные слова и пунктуационные знаки - токены.\n",
    "#Модуль tokenize этой библиотеки содержит функцию tokenize(), которая принимает на вход текст и возвращает список токенов.\n",
    "#Токенизация - это процесс разбиения текста на отдельные слова и знаки препинания, который является первым шагом в обработке текста перед его анализом или классификацией.\n",
    "#Импортируя функцию tokenize() из библиотеки razdel, вы можете использовать ее для токенизации русскоязычных текстов на отдельные слова и пунктуационные знаки.\n",
    "#Результатом работы этой функции будет список объектов Token, каждый из которых содержит информацию о токене, такую как его текст, начальную и конечную позиции в исходном тексте.\n",
    "#Таким образом, этот модуль может быть использован для дальнейшей обработки текстовых данных, например, для извлечения ключевых слов или построения векторных представлений слов.\n",
    "\n",
    "import pymorphy2  # pip install pymorphy2\n",
    "#Библиотека pymorphy2 в Python является инструментом для морфологического анализа русскоязычных слов.\n",
    "#Она предоставляет возможность определения частей речи, склонения и спряжения слов, а также получения их нормальных форм.\n",
    "#Импортируя эту библиотеку с помощью команды import pymorphy2, вы можете использовать ее функционал для анализа и обработки русскоязычных слов.\n",
    "#Для этого необходимо создать объект класса MorphAnalyzer(), который является основным инструментом библиотеки pymorphy2.\n",
    "#Затем, вызывая различные методы этого объекта, можно производить морфологический анализ слов, например:\n",
    "#Получать нормальную форму слова\n",
    "#Определять его часть речи\n",
    "#Склонять и спрягать слово в нужной форме\n",
    "#Определять род, число и падеж слова\n",
    "#Получать различные грамматические характеристики слова\n",
    "#Этот модуль может быть использован для дальнейшей обработки текстовых данных, например, для лемматизации текста перед его анализом или классификацией,\n",
    "#а также для создания словарей и морфологических анализаторов для русского языка."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopword_ru = stopwords.words('russian')\n",
    "morph = pymorphy2.MorphAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "776"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('stopwords.txt') as f:\n",
    "    additional_stopwords = [w.strip() for w in f.readlines() if w]\n",
    "stopword_ru += additional_stopwords\n",
    "len(stopword_ru)\n",
    "#Этот код открывает файл 'stopwords.txt' в режиме чтения с помощью оператора with и связывает его с переменной f.\n",
    "#Затем он считывает содержимое файла, преобразует каждую строку в список слов, используя метод строк strip()\n",
    "#для удаления лишних пробелов в начале и конце строки,и добавляет каждое слово в список additional_stopwords.\n",
    "#Далее, код добавляет список additional_stopwords к списку stopword_ru, который является списком стоп-слов на русском языке.\n",
    "#Это позволяет расширить список стоп-слов, используемый в некотором приложении или алгоритме, за счет добавления дополнительных слов, указанных в файле 'stopwords.txt'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''\n",
    "    очистка текста\n",
    "    \n",
    "    на выходе очищеный текст\n",
    "    \n",
    "    '''\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    text = text.lower()\n",
    "    text = text.strip('\\n').strip('\\r').strip('\\t')\n",
    "    text = re.sub(\"-\\s\\r\\n\\|-\\s\\r\\n|\\r\\n\", '', str(text))\n",
    "\n",
    "    text = re.sub(\"[0-9]|[-—.,:;_%©«»?*!@#№$^•·&()]|[+=]|[[]|[]]|[/]|\", '', text)\n",
    "    text = re.sub(r\"\\r\\n\\t|\\n|\\\\s|\\r\\t|\\\\n\", ' ', text)\n",
    "    text = re.sub(r'[\\xad]|[\\s+]', ' ', text.strip())\n",
    "    \n",
    "    #tokens = list(tokenize(text))\n",
    "    #words = [_.text for _ in tokens]\n",
    "    #words = [w for w in words if w not in stopword_ru]\n",
    "    \n",
    "    #return \" \".join(words)\n",
    "    return text\n",
    "\n",
    "cache = {}\n",
    "\n",
    "def lemmatization(text):\n",
    "    '''\n",
    "    лемматизация\n",
    "        [0] если зашел тип не `str` делаем его `str`\n",
    "        [1] токенизация предложения через razdel\n",
    "        [2] проверка есть ли в начале слова '-'\n",
    "        [3] проверка токена с одного символа\n",
    "        [4] проверка есть ли данное слово в кэше\n",
    "        [5] лемматизация слова\n",
    "        [6] проверка на стоп-слова\n",
    "\n",
    "    на выходе лист отлемматизированых токенов\n",
    "    '''\n",
    "\n",
    "    # [0]\n",
    "    if not isinstance(text, str):\n",
    "        text = str(text)\n",
    "    \n",
    "    # [1]\n",
    "    tokens = list(tokenize(text))\n",
    "    words = [_.text for _ in tokens]\n",
    "\n",
    "    words_lem = []\n",
    "    for w in words:\n",
    "        if w[0] == '-': # [2]\n",
    "            w = w[1:]\n",
    "        if len(w)>1: # [3]\n",
    "            if w in cache: # [4]\n",
    "                words_lem.append(cache[w])\n",
    "            else: # [5]\n",
    "                temp_cach = cache[w] = morph.parse(w)[0].normal_form\n",
    "                words_lem.append(temp_cach)\n",
    "    \n",
    "    words_lem_without_stopwords=[i for i in words_lem if not i in stopword_ru] # [6]\n",
    "    \n",
    "    return words_lem_without_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 27.6 s\n",
      "Wall time: 27.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем очистку текста. Будет долго...\n",
    "news['title'] = news['title'].apply(lambda x: clean_text(x), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 3min 30s\n",
      "Wall time: 3min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "#Запускаем лемматизацию текста. Будет очень долго...\n",
    "news['title'] = news['title'].apply(lambda x: lemmatization(x), 1)\n",
    "#В этом коде цифра 1 указывает на ось, по которой функция apply() будет применяться к столбцу 'title' объекта DataFrame news.\n",
    "#Параметр axis метода apply() по умолчанию равен 0 и означает, что функция должна применяться к каждому столбцу,\n",
    "#но в данном случае он задан явно и равен 1, что означает, что функция должна применяться к каждой строке столбца 'title'.\n",
    "#Таким образом, функция clean_text() будет вызвана для каждого элемента столбца 'title' в DataFrame news и результат \n",
    "#ее выполнения будет сохранен в этом же столбце, заменяя исходные значения."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь в 3 строчки обучим нашу модель"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#сформируем список наших текстов, разбив еще и на пробелы\n",
    "texts = [t for t in news['title'].values]\n",
    "# Создать корпус из списка текстов\n",
    "common_dictionary = Dictionary(texts)\n",
    "common_corpus = [common_dictionary.doc2bow(text) for text in texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что такое common_dictionary и как он выглядит"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'выбросить'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_dictionary[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все просто - это словарь наших слов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запускаем обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import LdaModel\n",
    "# Обучим модель на корпусе.\n",
    "lda = LdaModel(common_corpus, num_topics=25, id2word=common_dictionary)#, passes=10)\n",
    "#Данный код использует библиотеку Gensim и класс LdaModel для обучения модели LDA (Latent Dirichlet Allocation) на заданном корпусе текстовых документов common_corpus.\n",
    "#LDA является вероятностной моделью тематического моделирования, которая позволяет выделить скрытые темы, представленные распределением слов в документах.\n",
    "#Она основана на предположении, что каждый документ в корпусе можно представить как смесь нескольких тем с различными вероятностями,\n",
    "#а каждая тема может быть представлена распределением слов.\n",
    "#В параметрах модели указывается количество тем num_topics, которые модель должна выделить, а также словарь id2word,\n",
    "#содержащий отображение между идентификаторами слов и их текстовыми представлениями.\n",
    "#При необходимости можно указать также дополнительные параметры, такие как количество проходов passes по корпусу.\n",
    "#После обучения модели LDA на заданном корпусе можно использовать ее для выделения тем в новых текстовых документах,\n",
    "#а также для анализа и интерпретации результатов, таких как топ-слова каждой темы или вероятности принадлежности документа к каждой теме."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import datapath\n",
    "# Сохранить модель на диск.\n",
    "temp_file = datapath(\"model.lda\")\n",
    "lda.save(temp_file)\n",
    "# Загрузите потенциально предварительно обученную модель с диска.\n",
    "lda = LdaModel.load(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучили модель. Теперь 2 вопроса:\n",
    "\n",
    "1. как выглядят наши темы\n",
    "2. как получить для документа вектор значений (вероятности принадлежности каждой теме)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['форвард', 'авангард', 'томаш', 'заборский', 'прокомментировать', 'игра', 'свой', 'команда', 'матч', 'чемпионат', 'кхл', 'против', 'атланта', 'nnnn', 'провести', 'плохой', 'матч', 'нижний', 'новгород', 'против', 'торпедо', 'настраиваться', 'первый', 'минута', 'включиться', 'работа', 'сказать', 'заборский', 'получиться', 'забросить', 'быстрый', 'гол', 'задать', 'хороший', 'темп', 'поединок', 'мочь', 'играть', 'ещё', 'хороший', 'сторона', 'пять', 'очко', 'выезд', 'девять', 'это', 'хороший']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(5, 0.087219715),\n",
       " (6, 0.06633881),\n",
       " (8, 0.429779),\n",
       " (12, 0.09568536),\n",
       " (20, 0.21605226),\n",
       " (21, 0.08776029)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Создайте новый корпус из ранее невиданных документов.\n",
    "other_texts = [t for t in news['title'].iloc[:3]]\n",
    "other_corpus = [common_dictionary.doc2bow(text) for text in other_texts]\n",
    "\n",
    "unseen_doc = other_corpus[2]\n",
    "print(other_texts[2])\n",
    "lda[unseen_doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic_0: эксперимент налог открыться платёжный сибирский понятие дальневосточный\n",
      "topic_1: газ турция турецкий приток улица аудитория метан\n",
      "topic_2: президент государство который это глава путин свой\n",
      "topic_3: высота восток золото искусство музыка поражать присудить\n",
      "topic_4: оборудование предмет отель ндс индонезия водитель колебаться\n",
      "topic_5: обнаружить человек тело взрыв произойти место город\n",
      "topic_6: год санкция который nn закон это китай\n",
      "topic_7: участок торговый фильм площадь документация скот выставочный\n",
      "topic_8: это который жизнь свой первый весь очень\n",
      "topic_9: проверка депутат товар который россия совет сообщество\n",
      "topic_10: пенсия возраст школа километр год страдать египет\n",
      "topic_11: год это который россия дело банк также\n",
      "topic_12: это который мочь всё весь свой говорить\n",
      "topic_13: исследование который nn данные мозг мочь источник\n",
      "topic_14: мальчик челябинский приступ пляж timesn выписка власов\n",
      "topic_15: ребёнок достигать британский подросток больной лётчик величина\n",
      "topic_16: компания смерть университет препарат спрос сократиться медицина\n",
      "topic_17: который фонд это nn россия свой следствие\n",
      "topic_18: фестиваль мероприятие обращение пресссекретарить пройти nn россия\n",
      "topic_19: военный сша ракета который научный год операция\n",
      "topic_20: год это который новый проект работа млрд\n",
      "topic_21: тур принадлежащий молдавия молдавский ильин пен матч\n",
      "topic_22: год рост млн цена рубль составить снижение\n",
      "topic_23: знаменитый озеро автобус мэй заключать актёр диск\n",
      "topic_24: мышь ввс кг латвия азербайджан литва чен\n"
     ]
    }
   ],
   "source": [
    "x=lda.show_topics(num_topics=25, num_words=7,formatted=False)\n",
    "topics_words = [(tp[0], [wd[0] for wd in tp[1]]) for tp in x]\n",
    "#Ниже код печатает только слова \n",
    "for topic,words in topics_words:\n",
    "    print(\"topic_{}: \".format(topic)+\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очень неплохо - большинство тем вполне можно описать о чем они"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте напишем функцию, которая будет нам возвращать векторное представление новости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text = news['title'].iloc[0]\n",
    "\n",
    "def get_lda_vector(text):\n",
    "    unseen_doc = common_dictionary.doc2bow(text)\n",
    "    lda_tuple = lda[unseen_doc]\n",
    "    not_null_topics = dict(zip([i[0] for i in lda_tuple], [i[1] for i in lda_tuple]))\n",
    "\n",
    "    output_vector = []\n",
    "    for i in range(25):\n",
    "        if i not in not_null_topics:\n",
    "            output_vector.append(0)\n",
    "        else:\n",
    "            output_vector.append(not_null_topics[i])\n",
    "    return np.array(output_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc_id</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.066326</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033079</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4896</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.099155</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.462696</td>\n",
       "      <td>0.058424</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.199080</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160612</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4897</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.087215</td>\n",
       "      <td>0.066407</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.429798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.216053</td>\n",
       "      <td>0.087759</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4898</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.669195</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040152</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4899</td>\n",
       "      <td>0.250821</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.290592</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.103548</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   doc_id   topic_0  topic_1   topic_2  topic_3  topic_4   topic_5   topic_6  \\\n",
       "0       6  0.000000      0.0  0.066326      0.0      0.0  0.000000  0.000000   \n",
       "1    4896  0.000000      0.0  0.099155      0.0      0.0  0.462696  0.058424   \n",
       "2    4897  0.000000      0.0  0.000000      0.0      0.0  0.087215  0.066407   \n",
       "3    4898  0.000000      0.0  0.000000      0.0      0.0  0.000000  0.000000   \n",
       "4    4899  0.250821      0.0  0.290592      0.0      0.0  0.000000  0.000000   \n",
       "\n",
       "   topic_7   topic_8  ...  topic_15  topic_16  topic_17  topic_18  topic_19  \\\n",
       "0      0.0  0.000000  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "1      0.0  0.199080  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "2      0.0  0.429798  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "3      0.0  0.669195  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "4      0.0  0.000000  ...       0.0       0.0       0.0       0.0       0.0   \n",
       "\n",
       "   topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "0  0.000000  0.000000  0.000000  0.033079       0.0  \n",
       "1  0.000000  0.160612  0.000000  0.000000       0.0  \n",
       "2  0.216053  0.087759  0.000000  0.000000       0.0  \n",
       "3  0.000000  0.040152  0.000000  0.000000       0.0  \n",
       "4  0.000000  0.000000  0.103548  0.000000       0.0  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topic_matrix = pd.DataFrame([get_lda_vector(text) for text in news['title'].values])\n",
    "topic_matrix.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "topic_matrix['doc_id'] = news['doc_id'].values\n",
    "topic_matrix = topic_matrix[['doc_id']+['topic_{}'.format(i) for i in range(25)]]\n",
    "topic_matrix.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Прекрасно, мы получили вектора наших новостей! И даже умеем интерпретировать получившиеся темы\\\n",
    "Можно двигаться далее"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Следующий шаг - векторные представления пользователей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>articles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>[293672, 293328, 293001, 293622, 293126, 1852]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>[3405, 1739, 2972, 1158, 1599, 322665]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>[1845, 2009, 2356, 1424, 2939, 323389]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid                                        articles\n",
       "0  u105138  [293672, 293328, 293001, 293622, 293126, 1852]\n",
       "1  u108690          [3405, 1739, 2972, 1158, 1599, 322665]\n",
       "2  u108339          [1845, 2009, 2356, 1424, 2939, 323389]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_dict = dict(zip(topic_matrix['doc_id'].values, topic_matrix[['topic_{}'.format(i) for i in range(25)]].values))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.03408372, 0.01808866, 0.        , 0.        , 0.        ,\n",
       "       0.        , 0.1191908 , 0.        , 0.13824035, 0.        ,\n",
       "       0.        , 0.17380114, 0.        , 0.08436631, 0.        ,\n",
       "       0.        , 0.02537937, 0.20842977, 0.07661167, 0.0371021 ,\n",
       "       0.02634536, 0.        , 0.        , 0.05045659, 0.        ])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc_dict[293622]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='label_of_your_choice'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.Самостоятельно разобраться с тем, что такое tfidf (документация https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html и еще - https://scikit-learn.org/stable/modules/feature_extraction.html#text-feature-extraction)\n",
    "\n",
    "\n",
    "TF-IDF (term frequency-inverse document frequency (частота термина, обратная частоте документа)) – это статистический показатель, используемый в информационном поиске и анализе текстовых данных. Он позволяет оценить важность слова в контексте документа или коллекции документов.\\\n",
    "TF-IDF вычисляется на основе двух показателей:\\\n",
    "TF (term frequency) – отношение числа вхождений слова к общему числу слов в документе.\\\n",
    "IDF (inverse document frequency) – логарифм отношения общего числа документов к числу документов, содержащих данное слово.\\\n",
    "Таким образом, TF-IDF для слова w в документе d рассчитывается по формуле:\n",
    "TF-IDF(w, d) = TF(w, d) * IDF(w)\n",
    "\n",
    "Векторизация текста на основе TF-IDF позволяет представить документы в виде набора чисел, каждое из которых представляет собой вес соответствующего слова в документе.\\\n",
    "Эти веса позволяют отранжировать документы по степени их сходства с запросом, что используется, например, в системах информационного поиска.\n",
    "\n",
    "В Python для вычисления TF-IDF можно использовать класс TfidfVectorizer из библиотеки scikit-learn.\\\n",
    "Этот класс позволяет вычислить матрицу TF-IDF для заданного набора текстовых документов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 2. Модифицировать код функции get_user_embedding таким образом, чтобы считалось не среднее (как в примере np.mean), а медиана. Применить такое преобразование к данным, обучить модель прогнозирования оттока и посчитать метрики качества и сохранить их: roc auc, precision/recall/f_score (для 3 последних - подобрать оптимальный порог с помощью precision_recall_curve, как это делалось на уроке)\n",
    "# вариант из лекции\n",
    "#user_articles_list = users['articles'].iloc[33]\n",
    "\n",
    "#def get_user_embedding(user_articles_list):\n",
    "#    user_articles_list = eval(user_articles_list)\n",
    "#    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "#    user_vector = np.mean(user_vector, 0)  # вариант из лекции - среднее\n",
    "#    return user_vector \n",
    "\n",
    "\n",
    "# вариант из лекции - среднеарифметическое\n",
    "#def get_user_embedding_mean(user_articles_list):\n",
    "#    user_articles_list = eval(user_articles_list)\n",
    "#    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "#    user_vector = np.mean(user_vector, 0)\n",
    "#    return user_vector\n",
    "# вариант  - среднее\n",
    "#def get_user_embedding_median(user_articles_list):\n",
    "#    user_articles_list = eval(user_articles_list)\n",
    "#    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "#    user_vector = np.median(user_vector, 0)\n",
    "#    return user_vector\n",
    "# вариант  - максимальное\n",
    "#def get_user_embedding_max(user_articles_list):\n",
    "#    user_articles_list = eval(user_articles_list)\n",
    "#    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "#    user_vector = np.max(user_vector, 0)\n",
    "#    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_embedding(user_articles_list, metric=np.mean):\n",
    "    \n",
    "    user_articles_list = eval(user_articles_list)\n",
    "    user_vector = np.array([doc_dict[doc_id] for doc_id in user_articles_list])\n",
    "    user_vector = metric(user_vector, 0)\n",
    "    \n",
    "    return user_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(metric): \n",
    "\n",
    "    user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x, metric), 1)])\n",
    "    user_embeddings.columns = [f'topic_{i}' for i in range(25)]\n",
    "    user_embeddings['uid'] = users['uid'].values\n",
    "    user_embeddings = user_embeddings[['uid']+[f'topic_{i}' for i in range(25)]]\n",
    "\n",
    "    X = pd.merge(user_embeddings, target, 'left')\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X[[f'topic_{i}' for i in range(25)]], \n",
    "                                                        X['churn'], random_state=0)\n",
    "\n",
    "    logreg = LogisticRegression()\n",
    "\n",
    "    logreg.fit(X_train, y_train)\n",
    "\n",
    "    preds = logreg.predict_proba(X_test)[:, 1]\n",
    "\n",
    "    precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "    fscore = (2 * precision * recall) / (precision + recall)\n",
    "    roc_auc = roc_auc_score(y_test, preds)\n",
    "\n",
    "    ix = np.argmax(fscore)\n",
    "\n",
    "    return roc_auc, precision[ix], recall[ix], fscore[ix]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Повторить п.2, но используя уже не медиану, а max\n",
    "\n",
    "\n",
    "#### 5. Сформировать на выходе единую таблицу, сравнивающую качество 3 разных метода получения эмбедингов пользователей: mean, median, max, idf_mean по метрикам roc_auc, precision, recall, f_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>median</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>roc_auc</th>\n",
       "      <td>0.941745</td>\n",
       "      <td>0.972219</td>\n",
       "      <td>0.974343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>precision</th>\n",
       "      <td>0.614094</td>\n",
       "      <td>0.768924</td>\n",
       "      <td>0.771084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recall</th>\n",
       "      <td>0.746939</td>\n",
       "      <td>0.787755</td>\n",
       "      <td>0.783673</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fscore</th>\n",
       "      <td>0.674033</td>\n",
       "      <td>0.778226</td>\n",
       "      <td>0.777328</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               mean    median       max\n",
       "roc_auc    0.941745  0.972219  0.974343\n",
       "precision  0.614094  0.768924  0.771084\n",
       "recall     0.746939  0.787755  0.783673\n",
       "fscore     0.674033  0.778226  0.777328"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metrics = {'mean': np.mean, 'median': np.median, 'max': np.max}\n",
    "stats = pd.DataFrame(columns=metrics.keys())\n",
    "\n",
    "for k, v in metrics.items():\n",
    "    roc_auc, precision, recall, fscore = get_metrics(v)\n",
    "    stats.loc[f'roc_auc', k] = roc_auc\n",
    "    stats.loc[f'precision', k] = precision\n",
    "    stats.loc[f'recall', k] = recall\n",
    "    stats.loc[f'fscore', k] = fscore\n",
    "    \n",
    "stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Сделать самостоятельные выводы и предположения о том, почему тот или ной способ оказался эффективнее остальных\n",
    "\n",
    "\n",
    "С увеличием порога вхождения все метрики качества стремительно растут."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### далее продолжение кода по лекции."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Интересовался новостями с топиками topic_3, topic_14 (что-то про политику и государство)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[323329, 321961, 324743, 323186, 324632, 474690]'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users['articles'].iloc[33]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'глава российский мид сергей лавров опровергнуть появиться сми информация якобы готовиться обмен декларация россия сша сотрудничество сфера сообщать риа новость nn читать сообщение разговаривать автор сообщение знать откуда автор источник какихлибо основание подобный род репортаж знать откуда информация появиться сказать журналист итог встреча госсекретарь сша джон керри nn позиция свой изложить декларация напринимать достаточно рамка обсе рамка совет россия нато высокий уровень продекларировать всё обеспечивать неделимость безопасность никто обеспечивать свой безопасность счёт безопасность продолжить министр слово лавров москва считать система нато создавать проблема наш безопасность поэтому декларация недостаточно мочь договариваться совместный система россия предлагать ещё начинать год президент путин посещать сша нужно вести речь очередной декларация гарантия который проверять объективный военнотехнический критерий гарантия ненаправленность система против российский ядерный потенциал подчеркнуть глава мид вторник газета коммерсантъ ссылаться дипломатический источник написать барак обама владимир путин выйти тупик обменяться политический декларация пообещать использовать свой потенциал друг против друг'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(news[news['doc_id']==323186]['title'].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь получим эмбединги для всех пользователей и проверим их качество на конкретной downstream-задаче"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>uid</th>\n",
       "      <th>topic_0</th>\n",
       "      <th>topic_1</th>\n",
       "      <th>topic_2</th>\n",
       "      <th>topic_3</th>\n",
       "      <th>topic_4</th>\n",
       "      <th>topic_5</th>\n",
       "      <th>topic_6</th>\n",
       "      <th>topic_7</th>\n",
       "      <th>topic_8</th>\n",
       "      <th>...</th>\n",
       "      <th>topic_15</th>\n",
       "      <th>topic_16</th>\n",
       "      <th>topic_17</th>\n",
       "      <th>topic_18</th>\n",
       "      <th>topic_19</th>\n",
       "      <th>topic_20</th>\n",
       "      <th>topic_21</th>\n",
       "      <th>topic_22</th>\n",
       "      <th>topic_23</th>\n",
       "      <th>topic_24</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>u105138</td>\n",
       "      <td>0.009421</td>\n",
       "      <td>0.014228</td>\n",
       "      <td>0.040398</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003546</td>\n",
       "      <td>0.074359</td>\n",
       "      <td>0.035124</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.033429</td>\n",
       "      <td>...</td>\n",
       "      <td>0.016315</td>\n",
       "      <td>0.004230</td>\n",
       "      <td>0.117030</td>\n",
       "      <td>0.036694</td>\n",
       "      <td>0.052311</td>\n",
       "      <td>0.179399</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021801</td>\n",
       "      <td>0.008409</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>u108690</td>\n",
       "      <td>0.001880</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093631</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.021277</td>\n",
       "      <td>0.046849</td>\n",
       "      <td>0.002868</td>\n",
       "      <td>0.028899</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004806</td>\n",
       "      <td>0.003431</td>\n",
       "      <td>0.048189</td>\n",
       "      <td>0.013626</td>\n",
       "      <td>0.026829</td>\n",
       "      <td>0.075029</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.014378</td>\n",
       "      <td>0.012813</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>u108339</td>\n",
       "      <td>0.002053</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.101173</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.093521</td>\n",
       "      <td>0.058707</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022930</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.020143</td>\n",
       "      <td>0.061343</td>\n",
       "      <td>0.013836</td>\n",
       "      <td>0.062732</td>\n",
       "      <td>0.080471</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.019639</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>u101138</td>\n",
       "      <td>0.003259</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.040212</td>\n",
       "      <td>0.005082</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.031610</td>\n",
       "      <td>0.003941</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.326751</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004481</td>\n",
       "      <td>0.020979</td>\n",
       "      <td>0.014311</td>\n",
       "      <td>0.114287</td>\n",
       "      <td>0.007393</td>\n",
       "      <td>0.047658</td>\n",
       "      <td>0.018708</td>\n",
       "      <td>0.010888</td>\n",
       "      <td>0.099998</td>\n",
       "      <td>0.007269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>u108248</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.069695</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.016852</td>\n",
       "      <td>0.024116</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.059302</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.006026</td>\n",
       "      <td>0.074648</td>\n",
       "      <td>0.012014</td>\n",
       "      <td>0.022132</td>\n",
       "      <td>0.066881</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.022019</td>\n",
       "      <td>0.002040</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       uid   topic_0   topic_1   topic_2   topic_3   topic_4   topic_5  \\\n",
       "0  u105138  0.009421  0.014228  0.040398  0.000000  0.003546  0.074359   \n",
       "1  u108690  0.001880  0.000000  0.093631  0.000000  0.000000  0.021277   \n",
       "2  u108339  0.002053  0.005642  0.101173  0.000000  0.000000  0.093521   \n",
       "3  u101138  0.003259  0.000000  0.040212  0.005082  0.000000  0.031610   \n",
       "4  u108248  0.000000  0.000000  0.069695  0.000000  0.000000  0.016852   \n",
       "\n",
       "    topic_6   topic_7   topic_8  ...  topic_15  topic_16  topic_17  topic_18  \\\n",
       "0  0.035124  0.000000  0.033429  ...  0.016315  0.004230  0.117030  0.036694   \n",
       "1  0.046849  0.002868  0.028899  ...  0.004806  0.003431  0.048189  0.013626   \n",
       "2  0.058707  0.000000  0.022930  ...  0.000000  0.020143  0.061343  0.013836   \n",
       "3  0.003941  0.000000  0.326751  ...  0.004481  0.020979  0.014311  0.114287   \n",
       "4  0.024116  0.000000  0.059302  ...  0.000000  0.006026  0.074648  0.012014   \n",
       "\n",
       "   topic_19  topic_20  topic_21  topic_22  topic_23  topic_24  \n",
       "0  0.052311  0.179399  0.000000  0.021801  0.008409  0.000000  \n",
       "1  0.026829  0.075029  0.000000  0.014378  0.012813  0.000000  \n",
       "2  0.062732  0.080471  0.000000  0.019639  0.000000  0.000000  \n",
       "3  0.007393  0.047658  0.018708  0.010888  0.099998  0.007269  \n",
       "4  0.022132  0.066881  0.000000  0.022019  0.002040  0.000000  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_embeddings = pd.DataFrame([i for i in users['articles'].apply(lambda x: get_user_embedding(x), 1)])\n",
    "user_embeddings.columns = ['topic_{}'.format(i) for i in range(25)]\n",
    "user_embeddings['uid'] = users['uid'].values\n",
    "user_embeddings = user_embeddings[['uid']+['topic_{}'.format(i) for i in range(25)]]\n",
    "user_embeddings.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Датасет готов - можно попробовать обучить модель. Загрузим нашу разметку"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = pd.read_csv(\"users_churn.csv\")\n",
    "target.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.merge(user_embeddings, target, 'left')\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#разделим данные на train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X[['topic_{}'.format(i) for i in range(25)]], \n",
    "                                                    X['churn'], random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression()\n",
    "#обучим наш пайплайн\n",
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#наши прогнозы для тестовой выборки\n",
    "preds = logreg.predict_proba(X_test)[:, 1]\n",
    "preds[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score, roc_auc_score, precision_score, classification_report, precision_recall_curve, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Рассчитаем Precision, Recall, F_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision, recall, thresholds = precision_recall_curve(y_test, preds)\n",
    "fscore = (2 * precision * recall) / (precision + recall)\n",
    "# locate the index of the largest f score\n",
    "ix = np.argmax(fscore)\n",
    "print('Best Threshold=%f, F-Score=%.3f, Precision=%.3f, Recall=%.3f' % (thresholds[ix], \n",
    "                                                                        fscore[ix],\n",
    "                                                                        precision[ix],\n",
    "                                                                        recall[ix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#мы уже нашли ранее \"оптимальный\" порог, когда максимизировали f_score\n",
    "font = {'size' : 15}\n",
    "\n",
    "plt.rc('font', **font)\n",
    "\n",
    "cnf_matrix = confusion_matrix(y_test, preds>thresholds[ix])\n",
    "plt.figure(figsize=(10, 8))\n",
    "plot_confusion_matrix(cnf_matrix, classes=['Non-Churn', 'churn'],\n",
    "                      title='Confusion matrix')\n",
    "plt.savefig(\"conf_matrix.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В целом мы видим, что получившиеся векторные представления содержат какой-то сигнал и позволяют решать нашу прикладную задачу. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ссылки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. http://www.machinelearning.ru/wiki/images/d/d5/Voron17survey-artm.pdf\n",
    "\n",
    "Для определения значимости признаков в наборе данных существует несколько методов, в том числе и методы выбора признаков, основанные на их важности. Один из таких методов – это метод исключения признаков (feature selection), который заключается в том, чтобы выбрать подмножество признаков, наиболее важных для предсказания целевой переменной, и исключить остальные.\n",
    "\n",
    "Один из подходов к выбору наиболее важных признаков – это использование моделей машинного обучения с возможностью оценки важности признаков. Например, для алгоритмов деревьев решений (decision trees) или случайного леса (random forest) можно вычислить важность каждого признака на основе его вклада в улучшение разбиения данных. Также существуют методы, основанные на регрессионных моделях, например, Lasso-регрессия (Least Absolute Shrinkage and Selection Operator), которая позволяет отбирать признаки, имеющие наибольший вклад в предсказание целевой переменной при ограниченной сложности модели.\n",
    "\n",
    "Кроме того, для определения важности признаков можно использовать статистические методы, такие как анализ дисперсии (ANOVA) или корреляционный анализ, которые позволяют оценить вклад каждого признака в общую дисперсию целевой переменной или взаимосвязь между признаками.\n",
    "\n",
    "Главное, что следует отметить, это то, что модели ARTM и BigARTM на данный момент являются одними из наиболее актуальных методов тематического моделирования текстов, позволяющих учитывать разнообразные факторы, такие как мультиязычность, специфические свойства документов и т.д. Они также имеют высокую скорость обработки и масштабируемость, что делает их привлекательными для использования в больших проектах.\n",
    "\n",
    "Кроме того, книга описывает некоторые новейшие исследования, связанные с применением моделей ARTM и BigARTM в различных областях, таких как анализ социальных сетей, медицинские исследования и другие. В целом, можно сказать, что тематическое моделирование текстов с использованием моделей ARTM и BigARTM продолжает развиваться и находить новые применения в различных областях.\n",
    "\n",
    "Книга \"BigARTM: сравнение моделей тематического моделирования на больших корпусах\" написана профессором В.В. Воронцовым и является обзором современных методов тематического моделирования, а также представляет новый метод под названием BigARTM.\n",
    "\n",
    "Основные знания из этой книги можно свести к следующему:\n",
    "\n",
    "Тематическое моделирование - это метод машинного обучения, который позволяет извлекать темы из больших наборов текстовых данных.\n",
    "\n",
    "Существует несколько подходов к тематическому моделированию, включая Latent Dirichlet Allocation (LDA), Probabilistic Latent Semantic Analysis (PLSA), и тематические модели с разреженным векторным представлением (sparse topic models).\n",
    "\n",
    "BigARTM - это новый метод тематического моделирования, который использует комбинацию различных моделей и позволяет работать с большими наборами данных.\n",
    "\n",
    "Для эффективного применения тематического моделирования важно правильно выбирать параметры модели и проводить ее оценку, в том числе с помощью метрик качества.\n",
    "\n",
    "Тематическое моделирование имеет широкий спектр приложений, включая анализ социальных сетей, обработку текстов на естественном языке, анализ клиентской обратной связи и многое другое.\n",
    "\n",
    "В целом, книга представляет обзор основных методов и инструментов тематического моделирования и является полезным источником информации для исследователей и практиков, работающих в области анализа текстовых данных.\n",
    "\n",
    "\n",
    "2. https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation\n",
    "\n",
    "\n",
    "\n",
    "Скрытое распределение Дирихле - это вероятностное распределение, используемое в статистическом моделировании для описания распределения вероятностей наблюдаемых переменных, которые являются смесью неизвестных распределений.\n",
    "\n",
    "В частности, скрытое распределение Дирихле может использоваться в задаче тематического моделирования, где необходимо определить скрытые темы, которые могут быть выявлены из набора текстовых документов. В этом случае каждый документ рассматривается как смесь различных тем, а каждая тема представляется скрытым распределением Дирихле.\n",
    "\n",
    "Скрытое распределение Дирихле имеет несколько параметров, которые могут быть оценены с использованием различных методов оптимизации, таких как EM-алгоритм или вариационный метод. Эти методы позволяют определить параметры распределения на основе наблюдаемых данных.\n",
    "\n",
    "Одно из применений скрытого распределения Дирихле - это модель LDA (Latent Dirichlet Allocation), которая используется для тематического моделирования. В этой модели каждый документ представляется как смесь нескольких скрытых тем, а каждая тема - как распределение Дирихле над словами.\n",
    "\n",
    "Скрытое распределение Дирихле является важным инструментом в статистическом моделировании и используется во многих других областях, таких как машинное обучение, байесовская статистика и теория информации.\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
